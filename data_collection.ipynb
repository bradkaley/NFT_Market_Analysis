{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4e8bb6a",
   "metadata": {},
   "source": [
    "# Data Collection & Cleaning Code for NFT Market Analysis Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62d26806",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initial imports\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import alpaca_trade_api as tradeapi\n",
    "import json\n",
    "import csv\n",
    "\n",
    "\n",
    "#Imports required for pulling data using OpenSea API and cleaning the data\n",
    "from helpers import parse_events_data, parse_assets_data, parse_sale_data, parse_listing_data\n",
    "from pandas_profiling import ProfileReport\n",
    "import pickle\n",
    "from statistics import *\n",
    "from scipy.stats import combine_pvalues\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "plt.style.use('ggplot')\n",
    "import time\n",
    "from datetime import date, timedelta, datetime\n",
    "import glob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "093e8eef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load API keys file\n",
    "load_dotenv(\"OUR_KEYS.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4b14d740",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Set Crypto Compare API keys for crypto data\n",
    "cryptocompare_api_key = os.getenv(\"CRYPTOCOMPARE_API_KEY\")\n",
    "type(cryptocompare_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "972b4cb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Set Alpaca API keys for US stock market data\n",
    "alpaca_api_key = os.getenv(\"ALPACA_API_KEY\")\n",
    "alpaca_secret_key = os.getenv(\"ALPACA_SECRET_KEY\")\n",
    "type(alpaca_api_key)\n",
    "type(alpaca_secret_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b31ed1c",
   "metadata": {},
   "source": [
    "## Pull Alpaca data using API, save in Pandas dataframe, and clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "111a8578-cadd-4836-8023-748f46054197",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Carl's code to go here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e354248d-195b-458c-bcd2-a6a9ef0cde62",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the dataframe with the Alpaca stock data as a csv file\n",
    "stocks_data.to_csv(path_or_buf='Output_csv_data_files/stocks_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c6f1c5",
   "metadata": {},
   "source": [
    "## Pull Cryptocompare data using API, save in Pandas dataframe, and clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "87587b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Carl's code to go here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c6b07f-f055-4922-8317-593e8549491d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the dataframe with the crypto data as a csv file\n",
    "crypto_data.to_csv(path_or_buf='Output_csv_data_files/crypto_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206c199d-fff7-43d0-96b5-1d3de3931822",
   "metadata": {},
   "source": [
    "## Import Ethereum gas price data, save in Pandas dataframe, and clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1affbdd3-da50-4522-a2bc-fb6091fdf8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#John's code to go here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40ac795-c627-494f-bca1-b5a5326417bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the dataframe with the gas price data as a csv file\n",
    "gas_price_data.to_csv(path_or_buf='Output_csv_data_files/gas_price_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3eba02f",
   "metadata": {},
   "source": [
    "## Pull OpenSea data using API, save in Pandas dataframe, and clean the data\n",
    "\n",
    "This section contains code pulled from Alex Duffy's Github which references base code by Adil Moujahid. \\\n",
    "Blog post located here: https://alxdfy.github.io/2021/09/19/data-mining-OpenSea_markdown.html#get-nft \\\n",
    "GitHub repository located here: https://github.com/AlxDfy/OpenSea_API_DataScience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "20ab14ed-7ded-4906-94a5-65707973f572",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set a variable equal to the smart contract address for the desired NFT collection - in this case we'll be evaluating the collection \"Cryptopunks\"\n",
    "asset_contract_address = \"0x18Df6C571F6fE9283B87f910E41dc5c8b77b7da5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6bc3c2ff-0f32-4f10-ac36-cd42d0884e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define function to pull NFT properties data from Opensea\n",
    "\n",
    "def download_asset_info(save_location):\n",
    "    if not os.path.isdir(save_location):\n",
    "        os.makedirs(save_location)\n",
    "    url = \"https://api.opensea.io/api/v1/assets\"\n",
    "    listoassets = []\n",
    "\n",
    "    for i in range(0, 3000):\n",
    "        querystring = {\"token_ids\":list(range((i*30), (i*30)+30)),\n",
    "                       \"asset_contract_address\":asset_contract_address,\n",
    "                       \"order_direction\":\"desc\",\n",
    "                       \"offset\":\"0\",\n",
    "                       \"limit\":\"30\"}\n",
    "        response = requests.request(\"GET\", url, params=querystring)\n",
    "\n",
    "        print(i, end=\" \")\n",
    "        if response.status_code != 200:\n",
    "            print('error')\n",
    "            print(response.json())\n",
    "            break\n",
    "\n",
    "        # Getting assets data\n",
    "        assets = response.json()['assets']\n",
    "        if assets == []:\n",
    "            break\n",
    "        # Parsing assets data\n",
    "        parsed_assets = [parse_assets_data(asset) for asset in assets]\n",
    "        # storing parsed events data into list\n",
    "        listoassets.append(parsed_assets)\n",
    "    \n",
    "    # Flatten everything into one list\n",
    "    listoassets = [item for sublist in listoassets for item in sublist]\n",
    "    # Convert to df\n",
    "    assets_df = pd.DataFrame(listoassets)\n",
    "\n",
    "    #Save data in a new file\n",
    "    with open(save_location + 'assets_df'+str(date.today())+r'.pkl', 'wb') as f:\n",
    "        pickle.dump(assets_df, f)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a3cc7293-cb6d-4d33-af0e-a7f77b562b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define function to pull NFT sales data from Opensea\n",
    "# Download sales info from start_date to end _date and save them all into their own day's files\n",
    "# Default values 07/30/21 to today\n",
    "\n",
    "def download_sales_info(save_location, start_date = date(2021, 7, 30), end_date = date.today()):\n",
    "    if not os.path.isdir(save_location):\n",
    "        os.makedirs(save_location)\n",
    "    url = \"https://api.opensea.io/api/v1/events\"\n",
    "    # get the number of days that we want to download and save sales for\n",
    "    delta = end_date - start_date\n",
    "    count_days = int(delta.days)\n",
    "    \n",
    "    for i in range(count_days+1):\n",
    "        sales_that_day = []\n",
    "        # set start and end of the day we are checking, if it's today set end to current time\n",
    "        if date.today() == (start_date + timedelta(days=i)):\n",
    "            before = datetime.now()\n",
    "            after = datetime.combine((start_date + timedelta(days=i)), datetime.min.time())\n",
    "        else:\n",
    "            before = datetime.combine((start_date + timedelta(days=i+1)), datetime.min.time())\n",
    "            after = datetime.combine((start_date + timedelta(days=i)), datetime.min.time())\n",
    "        # There are too many transactions, now have to break them up by chunks in the day\n",
    "        hour_chunks = 24\n",
    "        chunk_count = 24/hour_chunks\n",
    "        time.sleep(.5)\n",
    "\n",
    "        for chunk in range(int(chunk_count)):\n",
    "            end = False\n",
    "            for j in range(0, 35):\n",
    "                time.sleep(.5)\n",
    "                # add the hour_chunk to the start of the day (after) time for each chunk\n",
    "                # use the actual before if we pass it chronologically though\n",
    "                changed_before = after + timedelta(hours=hour_chunks*(chunk+1)) - timedelta(minutes = 1)\n",
    "                changed_after = after + timedelta(hours = hour_chunks*(chunk))\n",
    "                \n",
    "                # this should only happen on the last chunk of a split day or if on current day\n",
    "                if before < changed_before:\n",
    "                    changed_before = before\n",
    "                    end = True\n",
    "\n",
    "                querystring = {\"asset_contract_address\":asset_contract_address,\n",
    "                               \"event_type\":\"successful\",\n",
    "                               \"only_opensea\":\"true\",\n",
    "                               \"offset\":j*300,\n",
    "                               \"occurred_before\":changed_before,\n",
    "                               \"occurred_after\":changed_after,\n",
    "                               \"limit\":\"300\"}\n",
    "                headers = {\"Accept\": \"application/json\"}\n",
    "\n",
    "                response = requests.request(\"GET\", url, headers=headers, params=querystring)\n",
    "\n",
    "\n",
    "                print(j, end=\" \")\n",
    "                if response.status_code != 200:\n",
    "                    print('error')\n",
    "                    print(response.json())\n",
    "                    break\n",
    "\n",
    "                #Getting assets sales data\n",
    "                event_sales = response.json()['asset_events']\n",
    "\n",
    "                if event_sales == []:\n",
    "                    end =True\n",
    "                    break\n",
    "\n",
    "                # Parsing asset sales data\n",
    "                parsed_event_sales = [parse_sale_data(sale) for sale in event_sales]\n",
    "                # storing parsed events data into list\n",
    "                sales_that_day.append(parsed_event_sales)\n",
    "                # check if the last date in the list is the same day as \n",
    "                last_date = (datetime.strptime(parsed_event_sales[0]['timestamp'], '%Y-%m-%dT%H:%M:%S'))\n",
    "                print(last_date)\n",
    "            if end:\n",
    "                break\n",
    "        sales_that_day = [item for sublist in sales_that_day for item in sublist]\n",
    "        \n",
    "        print(str(len(sales_that_day))+ \" sales saved to\" + save_location + \"events_sales_list_\" + str(start_date + timedelta(days=i))+'.pkl')\n",
    "        with open(save_location + \"events_sales_list_\" + str(start_date + timedelta(days=i))+'.pkl', 'wb') as f:\n",
    "            pickle.dump(sales_that_day, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "af9027e2-96d7-4396-8d5b-607a83cb8956",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define function to pull NFT listings data from Opensea\n",
    "\n",
    "# Download listings info from start_date to end _date and save them all into their own day's files\n",
    "# Default values to first listing ever 7/30/21 to today\n",
    "def download_listings_info(save_location, start_date = date(2021, 7, 30), end_date = date.today()):\n",
    "    if not os.path.isdir(save_location):\n",
    "        os.makedirs(save_location)\n",
    "    url = \"https://api.opensea.io/api/v1/events\"\n",
    "    # get the number of days that we want to download and save listings for\n",
    "    delta = end_date - start_date\n",
    "    count_days = int(delta.days)\n",
    "    \n",
    "    for i in range(count_days+1):\n",
    "        listings_that_day = []\n",
    "        # set start and end of the day we are checking, if it's today set end to current time\n",
    "        if date.today() == (start_date + timedelta(days=i)):\n",
    "            before = datetime.now()\n",
    "            after = datetime.combine((start_date + timedelta(days=i)), datetime.min.time())\n",
    "        else:\n",
    "            before = datetime.combine((start_date + timedelta(days=i+1)), datetime.min.time())\n",
    "            after = datetime.combine((start_date + timedelta(days=i)), datetime.min.time())\n",
    "        # There are too many transactions, now have to break them up by chunks in the day\n",
    "        hour_chunks = 24\n",
    "        chunk_count = 24/hour_chunks\n",
    "        time.sleep(.5)\n",
    "\n",
    "        \n",
    "        for chunk in range(int(chunk_count)):\n",
    "            end = False\n",
    "            for j in range(0, 35):\n",
    "                time.sleep(.5)\n",
    "                # add the hour_chunk to the start of the day (after) time for each chunk\n",
    "                # use the actual before if we pass it chronologically though\n",
    "                changed_before = after + timedelta(hours=hour_chunks*(chunk+1)) - timedelta(minutes = 1)\n",
    "                changed_after = after + timedelta(hours = hour_chunks*(chunk))\n",
    "                \n",
    "                # this should only happen on the last chunk of a split day or if on current day\n",
    "                if before < changed_before:\n",
    "                    changed_before = before\n",
    "                    end = True\n",
    "\n",
    "                querystring = {\"asset_contract_address\":asset_contract_address,\n",
    "                               \"event_type\":\"created\",\n",
    "                               \"only_opensea\":\"true\",\n",
    "                               \"offset\":j*300,\n",
    "                               \"occurred_before\":changed_before,\n",
    "                               \"occurred_after\":changed_after,\n",
    "                               \"limit\":\"300\"}\n",
    "                headers = {\"Accept\": \"application/json\"}\n",
    "\n",
    "                response = requests.request(\"GET\", url, headers=headers, params=querystring)\n",
    "\n",
    "\n",
    "                print(j, end=\" \")\n",
    "                if response.status_code != 200:\n",
    "                    print('error')\n",
    "                    print(response.json())\n",
    "                    break\n",
    "\n",
    "                #Getting assets listings data\n",
    "                event_listings = response.json()['asset_events']\n",
    "\n",
    "                if event_listings == []:\n",
    "                    end = True\n",
    "                    break\n",
    "\n",
    "                # Parsing events listings data\n",
    "                parsed_event_listings = [parse_listing_data(listing) for listing in event_listings]\n",
    "                # storing parsed events data into list\n",
    "                listings_that_day.append(parsed_event_listings)\n",
    "                # check if the last date in the list is the same day as \n",
    "                print(parsed_event_listings[0]['created_date'])\n",
    "            if end:\n",
    "                break\n",
    "        listings_that_day = [item for sublist in listings_that_day for item in sublist]\n",
    "        \n",
    "        print(str(len(listings_that_day))+ \" listings saved to\" + save_location +\n",
    "              \"events_listings_list_\" + str(start_date + timedelta(days=i))+'.pkl')\n",
    "        with open(save_location + \"events_listings_list_\" + str(start_date + timedelta(days=i))+'.pkl', 'wb') as f:\n",
    "            pickle.dump(listings_that_day, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3088be-6066-48c1-9f10-b8644601ab71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1 2 3 4 5 6 7 8 "
     ]
    }
   ],
   "source": [
    "#Execute the pull requests for the properties and sales data for the NFTs\n",
    "\n",
    "# Change location to suit your own needs\n",
    "save_location = \"./static/animetas/\"\n",
    "\n",
    "# assets\n",
    "download_asset_info(save_location)\n",
    "\n",
    "# SALES\n",
    "# download sales info from start_date to end_date e.g. date(2021, 7, 30), date.today() - timedelta(days=1), etc.\n",
    "# defaults to first day of sales to today\n",
    "\n",
    "download_sales_info(save_location = save_location, start_date = date(2021,7,30))\n",
    "\n",
    "# LISTINGS\n",
    "# download listings info from start_date to end_date e.g. date(2021, 7, 30), date.today() - timedelta(days=1), etc.\n",
    "# defaults to first day of listings to today\n",
    "\n",
    "download_listings_info(save_location = save_location, start_date = date(2021,7,30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a43d448-359c-4a17-80b9-756275ae2280",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the sales lists, combine them, and turn into a DF\n",
    "def load_sales_info(save_location):\n",
    "    files = [filename for filename in os.listdir(save_location) if filename.startswith('events_sales')]\n",
    "    all_sales = []\n",
    "    # load all files for sales by day\n",
    "    for file in files:\n",
    "        with open(str(save_location) + str(file), 'rb') as f:\n",
    "            all_sales.append(pickle.load(f))\n",
    "    \n",
    "    # flatten into one list\n",
    "    all_sales = [item for sublist in all_sales for item in sublist]\n",
    "    # convert to dataframe\n",
    "    events_sales_df = pd.DataFrame(all_sales)\n",
    "    \n",
    "    return events_sales_df\n",
    "\n",
    "# load the listing lists, combine them, and turn into a DF\n",
    "def load_listings_info(save_location):\n",
    "    files = [filename for filename in os.listdir(save_location) if filename.startswith('events_listing')]\n",
    "    all_listings = []\n",
    "    # load all files for listings by day\n",
    "    for file in files:\n",
    "        with open(str(save_location) + str(file), 'rb') as f:\n",
    "            all_listings.append(pickle.load(f))\n",
    "    \n",
    "    # flatten into one list\n",
    "    all_listings = [item for sublist in all_listings for item in sublist]\n",
    "    # convert to dataframe\n",
    "    events_listings_df = pd.DataFrame(all_listings)\n",
    "    \n",
    "    return events_listings_df\n",
    "\n",
    "# load most recent saved assets df\n",
    "def load_assets_info(save_location):\n",
    "    files = glob.glob(str(save_location)+'assets_df????-??-??.pkl')\n",
    "    with open(max(files, key=os.path.getctime), 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e6c34914-50f1-4e28-823d-5f0c0c7c5e68",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'save_location' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_6968/1684327983.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m#SALES\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mevents_sales_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_sales_info\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msave_location\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;31m# Pre-processing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# Convert price from WEI to ETH & for now get rid of bundles and duplicates(?)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'save_location' is not defined"
     ]
    }
   ],
   "source": [
    "#load all our saved files\n",
    "\n",
    "#SALES\n",
    "events_sales_df = load_sales_info(save_location)\n",
    "# Pre-processing\n",
    "# Convert price from WEI to ETH & for now get rid of bundles and duplicates(?)\n",
    "events_sales_df = events_sales_df[(events_sales_df['payment_token'] != 'USDC') & (events_sales_df['is_bundle'] == False)].copy()\n",
    "events_sales_df = events_sales_df.loc[events_sales_df.astype(str).drop_duplicates().index]\n",
    "events_sales_df['total_price'] = events_sales_df['total_price']/10.**18\n",
    "# Change timestamp to datetime\n",
    "events_sales_df['timestamp'] = pd.to_datetime(events_sales_df['timestamp'])\n",
    "# Calculating the sale prices in USD\n",
    "events_sales_df['total_price_usd'] = events_sales_df['total_price'] * events_sales_df['usd_price']\n",
    "\n",
    "\n",
    "#LISTINGS\n",
    "events_listings_df = load_listings_info(save_location)\n",
    "# Pre-processing\n",
    "# Convert price from WEI to ETH & for now get rid of bundles and duplicates(?)\n",
    "events_listings_df = events_listings_df[(events_listings_df['payment_token'] != 'USDC') & (events_listings_df['is_bundle'] == False)].copy()\n",
    "events_listings_df = events_listings_df.loc[events_listings_df.astype(str).drop_duplicates().index]\n",
    "events_listings_df['starting_price'] = events_listings_df['starting_price']/10.**18\n",
    "# Change timestamp to datetime\n",
    "events_listings_df['created_date'] = pd.to_datetime(events_listings_df['created_date'])\n",
    "# Calculating the sale prices in USD\n",
    "events_listings_df['total_price_usd'] = events_listings_df['starting_price'] * events_listings_df['usd_price']\n",
    "\n",
    "\n",
    "#ASSETS\n",
    "assets_df = load_assets_info(save_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5f462f36-f3b5-47eb-9e9b-31a3391baba5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'assets_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_6968/660597047.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Save the dataframes with the assets, sales, and listings as a csv files\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0massets_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Output_csv_data_files/nft_assets_data.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mevents_sales_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Output_csv_data_files/nft_sales_data.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mevents_listings_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Output_csv_data_files/nft_listings_data.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'assets_df' is not defined"
     ]
    }
   ],
   "source": [
    "#Save the dataframes with the assets, sales, and listings as a csv files\n",
    "assets_df.to_csv(path_or_buf='Output_csv_data_files/nft_assets_data.csv')\n",
    "events_sales_df.to_csv(path_or_buf='Output_csv_data_files/nft_sales_data.csv')\n",
    "events_listings_df.to_csv(path_or_buf='Output_csv_data_files/nft_listings_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0219003b-3814-4168-8f75-6f9f6e2ea4ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f8f685-3abe-4cf0-8242-28ebc06dcb13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985fcb1e-2e58-401d-bade-973cdad5c779",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1118f752-423a-4d0a-a8e3-997d53d05aac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4b4c8e-7d44-41d9-9076-526fc6a4d582",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "457d831b",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (Temp/ipykernel_17844/4043542140.py, line 103)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\bkaley\\AppData\\Local\\Temp/ipykernel_17844/4043542140.py\"\u001b[1;36m, line \u001b[1;32m103\u001b[0m\n\u001b[1;33m    ]\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#Create list of top 100 NFT collections in OpenSea\n",
    "collections_list = [\n",
    "    \"cryptopunks\",\n",
    "    \"boredapeyachtclub\",\n",
    "    \"mutantapeyachtclub\",---\n",
    "    \"edificebybenkovach\",---\n",
    "    \"thesandbox\",\n",
    "    \"cosmiclabs\",\n",
    "    \"parallelalpha\",\n",
    "    \"divineanarchy\",\n",
    "    \"corruption(s*)\",---\n",
    "    \"artwars|aw\",---\n",
    "    \"neotokyoidentities\",---\n",
    "    \"cryptoadzbygremplin\",\n",
    "    \"coolcatsnft\",\n",
    "    \"neotokyopart2vaultcards\",---\n",
    "    \"decentraland\",\n",
    "    \"doodles-official\",\n",
    "    \"mekaverse\",\n",
    "    \"neotokyopart3itemcaches\",---\n",
    "    \"angryapesunited\",\n",
    "    \"cyberkongz\",\n",
    "    \"thedogepound\",---\n",
    "    \"therealgoatsociety\",\n",
    "    \"sipherianflash\",\n",
    "    \"desperateapewives\",---\n",
    "    \"playboyrabbitarsofficial\",---\n",
    "    \"theshiboshis\",\n",
    "    \"partyape|billionairesclub\",---\n",
    "    \"boredapechemistryclub\",---\n",
    "    \"treeverse\",\n",
    "    \"veefriends\",\n",
    "    \"fidenzabytylerhobbs\"---\n",
    "    \"meebits\",\n",
    "    \"emblemvault[ethereum]\",---\n",
    "    \"bearxlabs\",\n",
    "    \"smilesssvrs)\",\n",
    "    \"fatapeclub\",---\n",
    "    \"junglefreaksbytrosley\",\n",
    "    \"boredapekennelclub\",---\n",
    "    \"punkscomic\",---\n",
    "    \"bearsdeluxe\",---\n",
    "    \"kaijukingz\",\n",
    "    \"chainrunners\",\n",
    "    \"ringersbydmitricherniak\",---\n",
    "    \"cryptomories\",\n",
    "    \"lostpoets\",\n",
    "    \"voxcollectibles\",---\n",
    "    \"spookyboyscountryclub|byholyghost\",---\n",
    "    \"zedrun\",---\n",
    "    \"lazylions\",\n",
    "    \"chromiesquigglebysnowfro\",---\n",
    "    \"0n1force\",---\n",
    "    \"mutantcats\",\n",
    "    \"10ktfstockrom\",---\n",
    "    \"bossbeauties\",\n",
    "    \"worldofwomen\",\n",
    "    \"thorguards\",\n",
    "    \"eponymbyartai\",---\n",
    "    \"namewee4896collection\",\n",
    "    \"superfarmgenesisseries\",---\n",
    "    \"cryptovoxels\",\n",
    "    \"knownorigin\",---\n",
    "    \"flufworld\",---\n",
    "    \"ens\",\n",
    "    \"hor1zontroopers\",---\n",
    "    \"artblocksfactory\",---\n",
    "    \"boonjiproject\",\n",
    "    \"superrare\",\n",
    "    \"creatureworldnft\",---\n",
    "    \"supducks\",\n",
    "    \"elliotradesnftcollection\"---\n",
    "    \"themetakey\",---\n",
    "    \"digitalobjectsartwork\",---\n",
    "    \"twinflames\",\n",
    "    \"apeharmonymonsterclub\",\n",
    "    \"loot\",\n",
    "    \"quantum\",\n",
    "    \"headdao\",\n",
    "    \"thecurrency\",\n",
    "    \"pudgypenguins\",\n",
    "    \"dogesoundclubmates\",---\n",
    "    \"mastershaartist\",---\n",
    "    \"impacttheoryfounder'skey\",---\n",
    "    \"dogsofelon\",---\n",
    "    \"fewociousxrtfkt\",---\n",
    "    \"furballs\",\n",
    "    \"rtfktxjeffstaple\",---\n",
    "    \"adambombsquad\",---\n",
    "    \"cyberkongzvx\",---\n",
    "    \"non-fungiblefungimintpass\",---\n",
    "    \"officialwrappedmooncats(acclimated)\",---\n",
    "    \"mycuriocards\",---\n",
    "    \"mirandus\",\n",
    "    \"rarible\",\n",
    "    \"galaxyfightclub\",---\n",
    "    \"thefungiblebypak\",---\n",
    "    \"galacticapes\",\n",
    "    \"metaherouniverse\",\n",
    "    \"rtfktbonusitems\",---\n",
    "    \"sneakyvampiresyndicate\",---\n",
    "    \"2+2genesis\"---\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003e65ce-2f61-4f1d-bdf2-6e1e3c4afe7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create OpenSea API URL\n",
    "opensea_url = f\"https://api.opensea.io/api/v1/collection/{collections}/stats\"\n",
    "headers = {\"Accept\": \"application/json\"}\n",
    "\n",
    "#Create function to request data using API\n",
    "def get_data_by_collection(collections):\n",
    "    opensea_url = f\"https://api.opensea.io/api/v1/collection/{collections}/stats\"\n",
    "    opensea_response_data = requests.request(\"GET\", opensea_url, headers=headers).json()\n",
    "    return json.dumps([opensea_response_data], indent=4)\n",
    "\n",
    "#Loop through the list of NFT collections to pull data for each collection\n",
    "collection_data_list = []\n",
    "for c in collections_list:\n",
    "    collections_data = get_data_by_collection(c)\n",
    "    collection_data_list.append(collections_data)\n",
    "\n",
    "#Write the data to a csv file\n",
    "with open(\"collection_data_list\", 'w') as l:\n",
    "    write = csv.writer(l)\n",
    "    write.writerow(collection_data_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
