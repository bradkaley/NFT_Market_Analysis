{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2398b82",
   "metadata": {},
   "source": [
    "# NFT Market Analysis Project\n",
    "## Part 1: OpenSea NFT Data Collection & Initial Cleaning\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8fe2f7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initial imports\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import alpaca_trade_api as tradeapi\n",
    "import json\n",
    "import csv\n",
    "\n",
    "\n",
    "#Imports required for pulling data using OpenSea API and cleaning the data\n",
    "from helpers import parse_events_data, parse_assets_data, parse_sale_data, parse_listing_data\n",
    "from pandas_profiling import ProfileReport\n",
    "import pickle\n",
    "from statistics import *\n",
    "from scipy.stats import combine_pvalues\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "plt.style.use('ggplot')\n",
    "import time\n",
    "from datetime import date, timedelta, datetime\n",
    "import glob\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2f7841",
   "metadata": {},
   "source": [
    "## Pull OpenSea data using API, save in Pandas dataframe, and clean the data\n",
    "\n",
    "This section contains code pulled from Alex Duffy's Github which references base code by Adil Moujahid. \\\n",
    "Blog post located here: https://alxdfy.github.io/2021/09/19/data-mining-OpenSea_markdown.html#get-nft \\\n",
    "GitHub repository located here: https://github.com/AlxDfy/OpenSea_API_DataScience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da21b7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Set a variable equal to the smart contract address for the desired NFT collection\n",
    "Here we'll be evaluating the top 5 most popular (by volume) collections as of 11/16/2021: Bored Ape Yacht Club, Mutant Ape Yacht Club, CryptoPunks, The Sandbox, and Lil Baby Ape Club\n",
    "Each collection's smart contract address is swapped in for the \"asset_conctract_address variable to pull the data using the API and then saved as csv files. The smart contract addresses are as follows:\n",
    "Bored Ape Yacht Club: 0xBC4CA0EdA7647A8aB7C2061c2E118A18a936f13D\n",
    "Mutant Ape Yacht Club: 0x60E4d786628Fea6478F785A6d7e704777c86a7c6\n",
    "CryptoPunks: 0xb47e3cd837dDF8e4c57F05d70Ab865de6e193BBB\n",
    "The Sandbox: 0x50f5474724e0Ee42D9a4e711ccFB275809Fd6d4a\n",
    "Lil Baby Ape Club:0xB48eb7B72Ff5A4B5EF044EA9E706C990bb33884D\n",
    "\"\"\"\n",
    "\n",
    "asset_contract_address = \"0xb47e3cd837dDF8e4c57F05d70Ab865de6e193BBB\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "96d30b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define function to pull NFT properties data from Opensea\n",
    "\n",
    "def download_asset_info(save_location):\n",
    "    if not os.path.isdir(save_location):\n",
    "        os.makedirs(save_location)\n",
    "    url = \"https://api.opensea.io/api/v1/assets\"\n",
    "    listoassets = []\n",
    "\n",
    "    for i in range(0, 3000):\n",
    "        querystring = {\"token_ids\":list(range((i*30), (i*30)+30)),\n",
    "                       \"asset_contract_address\":asset_contract_address,\n",
    "                       \"order_direction\":\"desc\",\n",
    "                       \"offset\":\"0\",\n",
    "                       \"limit\":\"30\"}\n",
    "        response = requests.request(\"GET\", url, params=querystring)\n",
    "\n",
    "        print(i, end=\" \")\n",
    "        if response.status_code != 200:\n",
    "            print('error')\n",
    "            print(response.json())\n",
    "            break\n",
    "\n",
    "        # Getting assets data\n",
    "        assets = response.json()['assets']\n",
    "        if assets == []:\n",
    "            break\n",
    "        # Parsing assets data\n",
    "        parsed_assets = [parse_assets_data(asset) for asset in assets]\n",
    "        # storing parsed events data into list\n",
    "        listoassets.append(parsed_assets)\n",
    "    \n",
    "    # Flatten everything into one list\n",
    "    listoassets = [item for sublist in listoassets for item in sublist]\n",
    "    # Convert to df\n",
    "    assets_df = pd.DataFrame(listoassets)\n",
    "\n",
    "    #Save data in a new file\n",
    "    with open(save_location + 'assets_df'+str(date.today())+r'.pkl', 'wb') as f:\n",
    "        pickle.dump(assets_df, f)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "50245d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define function to pull NFT sales data from Opensea\n",
    "# Download sales info from start_date to end _date and save them all into their own day's files\n",
    "# Default values from October 2020 to today which captures the majority of the sales\n",
    "\n",
    "def download_sales_info(save_location, start_date = date(2020,10,1), end_date = date.today()):\n",
    "    if not os.path.isdir(save_location):\n",
    "        os.makedirs(save_location)\n",
    "    url = \"https://api.opensea.io/api/v1/events\"\n",
    "    # get the number of days that we want to download and save sales for\n",
    "    delta = end_date - start_date\n",
    "    count_days = int(delta.days)\n",
    "    \n",
    "    for i in range(count_days+1):\n",
    "        sales_that_day = []\n",
    "        # set start and end of the day we are checking, if it's today set end to current time\n",
    "        if date.today() == (start_date + timedelta(days=i)):\n",
    "            before = datetime.now()\n",
    "            after = datetime.combine((start_date + timedelta(days=i)), datetime.min.time())\n",
    "        else:\n",
    "            before = datetime.combine((start_date + timedelta(days=i+1)), datetime.min.time())\n",
    "            after = datetime.combine((start_date + timedelta(days=i)), datetime.min.time())\n",
    "        # There are too many transactions, now have to break them up by chunks in the day\n",
    "        hour_chunks = 24\n",
    "        chunk_count = 24/hour_chunks\n",
    "        time.sleep(.5)\n",
    "\n",
    "        for chunk in range(int(chunk_count)):\n",
    "            end = False\n",
    "            for j in range(0, 35):\n",
    "                time.sleep(.5)\n",
    "                # add the hour_chunk to the start of the day (after) time for each chunk\n",
    "                # use the actual before if we pass it chronologically though\n",
    "                changed_before = after + timedelta(hours=hour_chunks*(chunk+1)) - timedelta(minutes = 1)\n",
    "                changed_after = after + timedelta(hours = hour_chunks*(chunk))\n",
    "                \n",
    "                # this should only happen on the last chunk of a split day or if on current day\n",
    "                if before < changed_before:\n",
    "                    changed_before = before\n",
    "                    end = True\n",
    "\n",
    "                querystring = {\"asset_contract_address\":asset_contract_address,\n",
    "                               \"event_type\":\"successful\",\n",
    "                               \"only_opensea\":\"true\",\n",
    "                               \"offset\":j*300,\n",
    "                               \"occurred_before\":changed_before,\n",
    "                               \"occurred_after\":changed_after,\n",
    "                               \"limit\":\"300\"}\n",
    "                headers = {\"Accept\": \"application/json\"}\n",
    "\n",
    "                response = requests.request(\"GET\", url, headers=headers, params=querystring)\n",
    "\n",
    "\n",
    "                print(j, end=\" \")\n",
    "                if response.status_code != 200:\n",
    "                    print('error')\n",
    "                    print(response.json())\n",
    "                    break\n",
    "\n",
    "                #Getting assets sales data\n",
    "                event_sales = response.json()['asset_events']\n",
    "\n",
    "                if event_sales == []:\n",
    "                    end =True\n",
    "                    break\n",
    "\n",
    "                # Parsing asset sales data\n",
    "                parsed_event_sales = [parse_sale_data(sale) for sale in event_sales]\n",
    "                # storing parsed events data into list\n",
    "                sales_that_day.append(parsed_event_sales)\n",
    "                # check if the last date in the list is the same day as \n",
    "                last_date = (datetime.strptime(parsed_event_sales[0]['timestamp'], '%Y-%m-%dT%H:%M:%S'))\n",
    "                print(last_date)\n",
    "            if end:\n",
    "                break\n",
    "        sales_that_day = [item for sublist in sales_that_day for item in sublist]\n",
    "        \n",
    "        print(str(len(sales_that_day))+ \" sales saved to\" + save_location + \"events_sales_list_\" + str(start_date + timedelta(days=i))+'.pkl')\n",
    "        with open(save_location + \"events_sales_list_\" + str(start_date + timedelta(days=i))+'.pkl', 'wb') as f:\n",
    "            pickle.dump(sales_that_day, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8959b5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define function to pull NFT listings data from Opensea\n",
    "\n",
    "# Download listings info from start_date to end _date and save them all into their own day's files\n",
    "# Default values from October 2020 to today which captures the majority of the sales\n",
    "def download_listings_info(save_location, start_date = date(2020,10,1), end_date = date.today()):\n",
    "    if not os.path.isdir(save_location):\n",
    "        os.makedirs(save_location)\n",
    "    url = \"https://api.opensea.io/api/v1/events\"\n",
    "    # get the number of days that we want to download and save listings for\n",
    "    delta = end_date - start_date\n",
    "    count_days = int(delta.days)\n",
    "    \n",
    "    for i in range(count_days+1):\n",
    "        listings_that_day = []\n",
    "        # set start and end of the day we are checking, if it's today set end to current time\n",
    "        if date.today() == (start_date + timedelta(days=i)):\n",
    "            before = datetime.now()\n",
    "            after = datetime.combine((start_date + timedelta(days=i)), datetime.min.time())\n",
    "        else:\n",
    "            before = datetime.combine((start_date + timedelta(days=i+1)), datetime.min.time())\n",
    "            after = datetime.combine((start_date + timedelta(days=i)), datetime.min.time())\n",
    "        # There are too many transactions, now have to break them up by chunks in the day\n",
    "        hour_chunks = 24\n",
    "        chunk_count = 24/hour_chunks\n",
    "        time.sleep(.5)\n",
    "\n",
    "        \n",
    "        for chunk in range(int(chunk_count)):\n",
    "            end = False\n",
    "            for j in range(0, 35):\n",
    "                time.sleep(.5)\n",
    "                # add the hour_chunk to the start of the day (after) time for each chunk\n",
    "                # use the actual before if we pass it chronologically though\n",
    "                changed_before = after + timedelta(hours=hour_chunks*(chunk+1)) - timedelta(minutes = 1)\n",
    "                changed_after = after + timedelta(hours = hour_chunks*(chunk))\n",
    "                \n",
    "                # this should only happen on the last chunk of a split day or if on current day\n",
    "                if before < changed_before:\n",
    "                    changed_before = before\n",
    "                    end = True\n",
    "\n",
    "                querystring = {\"asset_contract_address\":asset_contract_address,\n",
    "                               \"event_type\":\"created\",\n",
    "                               \"only_opensea\":\"true\",\n",
    "                               \"offset\":j*300,\n",
    "                               \"occurred_before\":changed_before,\n",
    "                               \"occurred_after\":changed_after,\n",
    "                               \"limit\":\"300\"}\n",
    "                headers = {\"Accept\": \"application/json\"}\n",
    "\n",
    "                response = requests.request(\"GET\", url, headers=headers, params=querystring)\n",
    "\n",
    "\n",
    "                print(j, end=\" \")\n",
    "                if response.status_code != 200:\n",
    "                    print('error')\n",
    "                    print(response.json())\n",
    "                    break\n",
    "\n",
    "                #Getting assets listings data\n",
    "                event_listings = response.json()['asset_events']\n",
    "\n",
    "                if event_listings == []:\n",
    "                    end = True\n",
    "                    break\n",
    "\n",
    "                # Parsing events listings data\n",
    "                parsed_event_listings = [parse_listing_data(listing) for listing in event_listings]\n",
    "                # storing parsed events data into list\n",
    "                listings_that_day.append(parsed_event_listings)\n",
    "                # check if the last date in the list is the same day as \n",
    "                print(parsed_event_listings[0]['created_date'])\n",
    "            if end:\n",
    "                break\n",
    "        listings_that_day = [item for sublist in listings_that_day for item in sublist]\n",
    "        \n",
    "        print(str(len(listings_that_day))+ \" listings saved to\" + save_location +\n",
    "              \"events_listings_list_\" + str(start_date + timedelta(days=i))+'.pkl')\n",
    "        with open(save_location + \"events_listings_list_\" + str(start_date + timedelta(days=i))+'.pkl', 'wb') as f:\n",
    "            pickle.dump(listings_that_day, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e326b1f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1 2 3 4 5 6 7 8 9 10 "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_21236/4160412719.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# assets\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mdownload_asset_info\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msave_location\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m# SALES\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_21236/1992227203.py\u001b[0m in \u001b[0;36mdownload_asset_info\u001b[1;34m(save_location)\u001b[0m\n\u001b[0;32m     13\u001b[0m                        \u001b[1;34m\"offset\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;34m\"0\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m                        \"limit\":\"30\"}\n\u001b[1;32m---> 15\u001b[1;33m         \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"GET\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mquerystring\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\" \"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\api.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[1;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    540\u001b[0m         }\n\u001b[0;32m    541\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 542\u001b[1;33m         \u001b[0mresp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    543\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    653\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    654\u001b[0m         \u001b[1;31m# Send the request\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 655\u001b[1;33m         \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    656\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    657\u001b[0m         \u001b[1;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\adapters.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    437\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    438\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mchunked\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 439\u001b[1;33m                 resp = conn.urlopen(\n\u001b[0m\u001b[0;32m    440\u001b[0m                     \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    441\u001b[0m                     \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    697\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    698\u001b[0m             \u001b[1;31m# Make the request on the httplib connection object.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 699\u001b[1;33m             httplib_response = self._make_request(\n\u001b[0m\u001b[0;32m    700\u001b[0m                 \u001b[0mconn\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    701\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    443\u001b[0m                     \u001b[1;31m# Python 3 (including for exceptions like SystemExit).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    444\u001b[0m                     \u001b[1;31m# Otherwise it looks like a bug in the code.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 445\u001b[1;33m                     \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    446\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mSocketTimeout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBaseSSLError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSocketError\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    447\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_raise_timeout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout_value\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mread_timeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\packages\\six.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    438\u001b[0m                 \u001b[1;31m# Python 3\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    439\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 440\u001b[1;33m                     \u001b[0mhttplib_response\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    441\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    442\u001b[0m                     \u001b[1;31m# Remove the TypeError from the exception chain in\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36mgetresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1345\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1346\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1347\u001b[1;33m                 \u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1348\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1349\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36mbegin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    305\u001b[0m         \u001b[1;31m# read until we get a non-100 response\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    306\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 307\u001b[1;33m             \u001b[0mversion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    308\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    309\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36m_read_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    266\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    267\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_read_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 268\u001b[1;33m         \u001b[0mline\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"iso-8859-1\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    269\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    270\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mLineTooLong\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"status line\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    667\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    668\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 669\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    670\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    671\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1239\u001b[0m                   \u001b[1;34m\"non-zero flags not allowed in calls to recv_into() on %s\"\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1240\u001b[0m                   self.__class__)\n\u001b[1;32m-> 1241\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1242\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1243\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\ssl.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1097\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1098\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1099\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1100\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1101\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Execute the pull requests for the properties and sales data for the NFTs\n",
    "\n",
    "# Change location to suit your own needs\n",
    "save_location = \"./static/cryptopunks/\"\n",
    "\n",
    "# assets\n",
    "download_asset_info(save_location)\n",
    "\n",
    "# SALES\n",
    "# download sales info from start_date to end_date e.g. date(2020,10,1), date.today() - timedelta(days=1), etc.\n",
    "# from October 2020 to today which captures the majority of the sales\n",
    "\n",
    "download_sales_info(save_location = save_location, start_date = date(2020,10,1))\n",
    "\n",
    "# LISTINGS\n",
    "# download listings info from start_date to end_date e.g. date(2020,10,1), date.today() - timedelta(days=1), etc.\n",
    "# from October 2020 to today which captures the majority of the sales\n",
    "\n",
    "download_listings_info(save_location = save_location, start_date = date(2020,10,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6b0e9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the sales lists, combine them, and turn into a DF\n",
    "def load_sales_info(save_location):\n",
    "    files = [filename for filename in os.listdir(save_location) if filename.startswith('events_sales')]\n",
    "    all_sales = []\n",
    "    # load all files for sales by day\n",
    "    for file in files:\n",
    "        with open(str(save_location) + str(file), 'rb') as f:\n",
    "            all_sales.append(pickle.load(f))\n",
    "    \n",
    "    # flatten into one list\n",
    "    all_sales = [item for sublist in all_sales for item in sublist]\n",
    "    # convert to dataframe\n",
    "    events_sales_df = pd.DataFrame(all_sales)\n",
    "    \n",
    "    return events_sales_df\n",
    "\n",
    "# load the listing lists, combine them, and turn into a DF\n",
    "def load_listings_info(save_location):\n",
    "    files = [filename for filename in os.listdir(save_location) if filename.startswith('events_listing')]\n",
    "    all_listings = []\n",
    "    # load all files for listings by day\n",
    "    for file in files:\n",
    "        with open(str(save_location) + str(file), 'rb') as f:\n",
    "            all_listings.append(pickle.load(f))\n",
    "    \n",
    "    # flatten into one list\n",
    "    all_listings = [item for sublist in all_listings for item in sublist]\n",
    "    # convert to dataframe\n",
    "    events_listings_df = pd.DataFrame(all_listings)\n",
    "    \n",
    "    return events_listings_df\n",
    "\n",
    "# load most recent saved assets df\n",
    "def load_assets_info(save_location):\n",
    "    files = glob.glob(str(save_location)+'assets_df????-??-??.pkl')\n",
    "    with open(max(files, key=os.path.getctime), 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b5696f1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'save_location' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_21236/1551040261.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m#SALES\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mevents_sales_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_sales_info\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msave_location\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;31m# Pre-processing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# Convert price from WEI to ETH & for now get rid of bundles and duplicates(?)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'save_location' is not defined"
     ]
    }
   ],
   "source": [
    "#load all our saved files\n",
    "\n",
    "#SALES\n",
    "events_sales_df = load_sales_info(save_location)\n",
    "# Pre-processing\n",
    "# Convert price from WEI to ETH & for now get rid of bundles and duplicates(?)\n",
    "events_sales_df = events_sales_df[(events_sales_df['payment_token'] != 'USDC') & (events_sales_df['is_bundle'] == False)].copy()\n",
    "events_sales_df = events_sales_df.loc[events_sales_df.astype(str).drop_duplicates().index]\n",
    "events_sales_df['total_price'] = events_sales_df['total_price']/10.**18\n",
    "# Change timestamp to datetime\n",
    "events_sales_df['timestamp'] = pd.to_datetime(events_sales_df['timestamp'])\n",
    "# Calculating the sale prices in USD\n",
    "events_sales_df['total_price_usd'] = events_sales_df['total_price'] * events_sales_df['usd_price']\n",
    "\n",
    "\n",
    "#LISTINGS\n",
    "events_listings_df = load_listings_info(save_location)\n",
    "# Pre-processing\n",
    "# Convert price from WEI to ETH & for now get rid of bundles and duplicates(?)\n",
    "events_listings_df = events_listings_df[(events_listings_df['payment_token'] != 'USDC') & (events_listings_df['is_bundle'] == False)].copy()\n",
    "events_listings_df = events_listings_df.loc[events_listings_df.astype(str).drop_duplicates().index]\n",
    "events_listings_df['starting_price'] = events_listings_df['starting_price']/10.**18\n",
    "# Change timestamp to datetime\n",
    "events_listings_df['created_date'] = pd.to_datetime(events_listings_df['created_date'])\n",
    "# Calculating the sale prices in USD\n",
    "events_listings_df['total_price_usd'] = events_listings_df['starting_price'] * events_listings_df['usd_price']\n",
    "\n",
    "#ASSETS\n",
    "assets_df = load_assets_info(save_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8e79f0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the dataframes with the assets, sales, and listings as a csv files\n",
    "assets_df.to_csv(path_or_buf='Output_csv_data_files/nfts/assets.csv')\n",
    "events_sales_df.to_csv(path_or_buf='Output_csv_data_files/nfts/sales.csv')\n",
    "events_listings_df.to_csv(path_or_buf='Output_csv_data_files/nfts/listings.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21cb722b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
