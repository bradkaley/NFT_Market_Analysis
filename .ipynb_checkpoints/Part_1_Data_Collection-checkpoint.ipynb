{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ab3ada3",
   "metadata": {},
   "source": [
    "# NFT Market Analysis Project\n",
    "## Part 1: OpenSea NFT Data Collection & Initial Cleaning\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fcbc036c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas_profiling'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-4370a17388a5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;31m#Imports required for pulling data using OpenSea API and cleaning the data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mhelpers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mparse_events_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparse_assets_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparse_sale_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparse_listing_data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mpandas_profiling\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mProfileReport\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mstatistics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pandas_profiling'"
     ]
    }
   ],
   "source": [
    "#Initial imports\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import alpaca_trade_api as tradeapi\n",
    "import json\n",
    "import csv\n",
    "\n",
    "\n",
    "#Imports required for pulling data using OpenSea API and cleaning the data\n",
    "from helpers import parse_events_data, parse_assets_data, parse_sale_data, parse_listing_data\n",
    "from pandas_profiling import ProfileReport\n",
    "import pickle\n",
    "from statistics import *\n",
    "from scipy.stats import combine_pvalues\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "plt.style.use('ggplot')\n",
    "import time\n",
    "from datetime import date, timedelta, datetime\n",
    "import glob\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9468884a",
   "metadata": {},
   "source": [
    "## Pull OpenSea data using API, save in Pandas dataframe, and clean the data\n",
    "\n",
    "This section contains code pulled from Alex Duffy's Github which references base code by Adil Moujahid. \\\n",
    "Blog post located here: https://alxdfy.github.io/2021/09/19/data-mining-OpenSea_markdown.html#get-nft \\\n",
    "GitHub repository located here: https://github.com/AlxDfy/OpenSea_API_DataScience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02b4ea57",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Set a variable equal to the smart contract address for the desired NFT collection\n",
    "Here we'll be evaluating the 4 of the most popular (by volume) collections as of 11/16/2021: Bored Ape Yacht Club, Mutant Ape Yacht Club, The Sandbox, and Lil Baby Ape Club\n",
    "Each collection's smart contract address is swapped in for the \"asset_conctract_address variable to pull the data using the API and then saved as csv files. The smart contract addresses are as follows:\n",
    "Bored Ape Yacht Club: 0xBC4CA0EdA7647A8aB7C2061c2E118A18a936f13D\n",
    "Mutant Ape Yacht Club: 0x60E4d786628Fea6478F785A6d7e704777c86a7c6\n",
    "The Sandbox: 0x50f5474724e0Ee42D9a4e711ccFB275809Fd6d4a\n",
    "Lil Baby Ape Club:0xB48eb7B72Ff5A4B5EF044EA9E706C990bb33884D\n",
    "\"\"\"\n",
    "\n",
    "asset_contract_address = \"0xBC4CA0EdA7647A8aB7C2061c2E118A18a936f13D\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b65d2a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define function to pull NFT properties data from Opensea\n",
    "\n",
    "def download_asset_info(save_location):\n",
    "    if not os.path.isdir(save_location):\n",
    "        os.makedirs(save_location)\n",
    "    url = \"https://api.opensea.io/api/v1/assets\"\n",
    "    listoassets = []\n",
    "\n",
    "    for i in range(0, 3000):\n",
    "        querystring = {\"token_ids\":list(range((i*30), (i*30)+30)),\n",
    "                       \"asset_contract_address\":asset_contract_address,\n",
    "                       \"order_direction\":\"desc\",\n",
    "                       \"offset\":\"0\",\n",
    "                       \"limit\":\"30\"}\n",
    "        response = requests.request(\"GET\", url, params=querystring)\n",
    "\n",
    "        print(i, end=\" \")\n",
    "        if response.status_code != 200:\n",
    "            print('error')\n",
    "            print(response.json())\n",
    "            break\n",
    "\n",
    "        # Getting assets data\n",
    "        assets = response.json()['assets']\n",
    "        if assets == []:\n",
    "            break\n",
    "        # Parsing assets data\n",
    "        parsed_assets = [parse_assets_data(asset) for asset in assets]\n",
    "        # storing parsed events data into list\n",
    "        listoassets.append(parsed_assets)\n",
    "    \n",
    "    # Flatten everything into one list\n",
    "    listoassets = [item for sublist in listoassets for item in sublist]\n",
    "    # Convert to df\n",
    "    assets_df = pd.DataFrame(listoassets)\n",
    "\n",
    "    #Save data in a new file\n",
    "    with open(save_location + 'assets_df'+str(date.today())+r'.pkl', 'wb') as f:\n",
    "        pickle.dump(assets_df, f)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e9be7874",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define function to pull NFT sales data from Opensea\n",
    "# Download sales info from start_date to end _date and save them all into their own day's files\n",
    "# Default values from October 2020 to today which captures the majority of the sales\n",
    "\n",
    "def download_sales_info(save_location, start_date = date(2021,7,30), end_date = date.today()):\n",
    "    if not os.path.isdir(save_location):\n",
    "        os.makedirs(save_location)\n",
    "    url = \"https://api.opensea.io/api/v1/events\"\n",
    "    # get the number of days that we want to download and save sales for\n",
    "    delta = end_date - start_date\n",
    "    count_days = int(delta.days)\n",
    "    \n",
    "    for i in range(count_days+1):\n",
    "        sales_that_day = []\n",
    "        # set start and end of the day we are checking, if it's today set end to current time\n",
    "        if date.today() == (start_date + timedelta(days=i)):\n",
    "            before = datetime.now()\n",
    "            after = datetime.combine((start_date + timedelta(days=i)), datetime.min.time())\n",
    "        else:\n",
    "            before = datetime.combine((start_date + timedelta(days=i+1)), datetime.min.time())\n",
    "            after = datetime.combine((start_date + timedelta(days=i)), datetime.min.time())\n",
    "        # There are too many transactions, now have to break them up by chunks in the day\n",
    "        hour_chunks = 24\n",
    "        chunk_count = 24/hour_chunks\n",
    "        time.sleep(.5)\n",
    "\n",
    "        for chunk in range(int(chunk_count)):\n",
    "            end = False\n",
    "            for j in range(0, 35):\n",
    "                time.sleep(.5)\n",
    "                # add the hour_chunk to the start of the day (after) time for each chunk\n",
    "                # use the actual before if we pass it chronologically though\n",
    "                changed_before = after + timedelta(hours=hour_chunks*(chunk+1)) - timedelta(minutes = 1)\n",
    "                changed_after = after + timedelta(hours = hour_chunks*(chunk))\n",
    "                \n",
    "                # this should only happen on the last chunk of a split day or if on current day\n",
    "                if before < changed_before:\n",
    "                    changed_before = before\n",
    "                    end = True\n",
    "\n",
    "                querystring = {\"asset_contract_address\":asset_contract_address,\n",
    "                               \"event_type\":\"successful\",\n",
    "                               \"only_opensea\":\"true\",\n",
    "                               \"offset\":j*300,\n",
    "                               \"occurred_before\":changed_before,\n",
    "                               \"occurred_after\":changed_after,\n",
    "                               \"limit\":\"300\"}\n",
    "                headers = {\"Accept\": \"application/json\"}\n",
    "\n",
    "                response = requests.request(\"GET\", url, headers=headers, params=querystring)\n",
    "\n",
    "\n",
    "                print(j, end=\" \")\n",
    "                if response.status_code != 200:\n",
    "                    print('error')\n",
    "                    print(response.json())\n",
    "                    break\n",
    "\n",
    "                #Getting assets sales data\n",
    "                event_sales = response.json()['asset_events']\n",
    "\n",
    "                if event_sales == []:\n",
    "                    end =True\n",
    "                    break\n",
    "\n",
    "                # Parsing asset sales data\n",
    "                parsed_event_sales = [parse_sale_data(sale) for sale in event_sales]\n",
    "                # storing parsed events data into list\n",
    "                sales_that_day.append(parsed_event_sales)\n",
    "                # check if the last date in the list is the same day as \n",
    "                last_date = (datetime.strptime(parsed_event_sales[0]['timestamp'], '%Y-%m-%dT%H:%M:%S'))\n",
    "                print(last_date)\n",
    "            if end:\n",
    "                break\n",
    "        sales_that_day = [item for sublist in sales_that_day for item in sublist]\n",
    "        \n",
    "        print(str(len(sales_that_day))+ \" sales saved to\" + save_location + \"events_sales_list_\" + str(start_date + timedelta(days=i))+'.pkl')\n",
    "        with open(save_location + \"events_sales_list_\" + str(start_date + timedelta(days=i))+'.pkl', 'wb') as f:\n",
    "            pickle.dump(sales_that_day, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4c8bc006",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define function to pull NFT listings data from Opensea\n",
    "\n",
    "# Download listings info from start_date to end _date and save them all into their own day's files\n",
    "# Default values from October 2020 to today which captures the majority of the sales\n",
    "def download_listings_info(save_location, start_date = date(2021,7,30), end_date = date.today()):\n",
    "    if not os.path.isdir(save_location):\n",
    "        os.makedirs(save_location)\n",
    "    url = \"https://api.opensea.io/api/v1/events\"\n",
    "    # get the number of days that we want to download and save listings for\n",
    "    delta = end_date - start_date\n",
    "    count_days = int(delta.days)\n",
    "    \n",
    "    for i in range(count_days+1):\n",
    "        listings_that_day = []\n",
    "        # set start and end of the day we are checking, if it's today set end to current time\n",
    "        if date.today() == (start_date + timedelta(days=i)):\n",
    "            before = datetime.now()\n",
    "            after = datetime.combine((start_date + timedelta(days=i)), datetime.min.time())\n",
    "        else:\n",
    "            before = datetime.combine((start_date + timedelta(days=i+1)), datetime.min.time())\n",
    "            after = datetime.combine((start_date + timedelta(days=i)), datetime.min.time())\n",
    "        # There are too many transactions, now have to break them up by chunks in the day\n",
    "        hour_chunks = 24\n",
    "        chunk_count = 24/hour_chunks\n",
    "        time.sleep(.5)\n",
    "\n",
    "        \n",
    "        for chunk in range(int(chunk_count)):\n",
    "            end = False\n",
    "            for j in range(0, 35):\n",
    "                time.sleep(.5)\n",
    "                # add the hour_chunk to the start of the day (after) time for each chunk\n",
    "                # use the actual before if we pass it chronologically though\n",
    "                changed_before = after + timedelta(hours=hour_chunks*(chunk+1)) - timedelta(minutes = 1)\n",
    "                changed_after = after + timedelta(hours = hour_chunks*(chunk))\n",
    "                \n",
    "                # this should only happen on the last chunk of a split day or if on current day\n",
    "                if before < changed_before:\n",
    "                    changed_before = before\n",
    "                    end = True\n",
    "\n",
    "                querystring = {\"asset_contract_address\":asset_contract_address,\n",
    "                               \"event_type\":\"created\",\n",
    "                               \"only_opensea\":\"true\",\n",
    "                               \"offset\":j*300,\n",
    "                               \"occurred_before\":changed_before,\n",
    "                               \"occurred_after\":changed_after,\n",
    "                               \"limit\":\"300\"}\n",
    "                headers = {\"Accept\": \"application/json\"}\n",
    "\n",
    "                response = requests.request(\"GET\", url, headers=headers, params=querystring)\n",
    "\n",
    "\n",
    "                print(j, end=\" \")\n",
    "                if response.status_code != 200:\n",
    "                    print('error')\n",
    "                    print(response.json())\n",
    "                    break\n",
    "\n",
    "                #Getting assets listings data\n",
    "                event_listings = response.json()['asset_events']\n",
    "\n",
    "                if event_listings == []:\n",
    "                    end = True\n",
    "                    break\n",
    "\n",
    "                # Parsing events listings data\n",
    "                parsed_event_listings = [parse_listing_data(listing) for listing in event_listings]\n",
    "                # storing parsed events data into list\n",
    "                listings_that_day.append(parsed_event_listings)\n",
    "                # check if the last date in the list is the same day as \n",
    "                print(parsed_event_listings[0]['created_date'])\n",
    "            if end:\n",
    "                break\n",
    "        listings_that_day = [item for sublist in listings_that_day for item in sublist]\n",
    "        \n",
    "        print(str(len(listings_that_day))+ \" listings saved to\" + save_location +\n",
    "              \"events_listings_list_\" + str(start_date + timedelta(days=i))+'.pkl')\n",
    "        with open(save_location + \"events_listings_list_\" + str(start_date + timedelta(days=i))+'.pkl', 'wb') as f:\n",
    "            pickle.dump(listings_that_day, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3dae5b41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 0 error\n"
     ]
    },
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 1 (char 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_17144/3367999394.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;31m# from October 2020 to today which captures the majority of the sales\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mdownload_sales_info\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msave_location\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msave_location\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart_date\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2021\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;31m# LISTINGS\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_17144/291305080.py\u001b[0m in \u001b[0;36mdownload_sales_info\u001b[1;34m(save_location, start_date, end_date)\u001b[0m\n\u001b[0;32m     54\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m200\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'error'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 56\u001b[1;33m                     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     57\u001b[0m                     \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\models.py\u001b[0m in \u001b[0;36mjson\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    908\u001b[0m                     \u001b[1;31m# used.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    909\u001b[0m                     \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 910\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mcomplexjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    911\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    912\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\json\\__init__.py\u001b[0m in \u001b[0;36mloads\u001b[1;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    355\u001b[0m             \u001b[0mparse_int\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mparse_float\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    356\u001b[0m             parse_constant is None and object_pairs_hook is None and not kw):\n\u001b[1;32m--> 357\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    358\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    359\u001b[0m         \u001b[0mcls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mJSONDecoder\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\json\\decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(self, s, _w)\u001b[0m\n\u001b[0;32m    335\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    336\u001b[0m         \"\"\"\n\u001b[1;32m--> 337\u001b[1;33m         \u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    338\u001b[0m         \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    339\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mend\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\json\\decoder.py\u001b[0m in \u001b[0;36mraw_decode\u001b[1;34m(self, s, idx)\u001b[0m\n\u001b[0;32m    353\u001b[0m             \u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscan_once\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    354\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 355\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mJSONDecodeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Expecting value\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    356\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)"
     ]
    }
   ],
   "source": [
    "#Execute the pull requests for the properties and sales data for the NFTs\n",
    "\n",
    "# Change location to suit your own needs\n",
    "save_location = \"./static/cryptopunks/\"\n",
    "\n",
    "# assets\n",
    "download_asset_info(save_location)\n",
    "\n",
    "# SALES\n",
    "# download sales info from start_date to end_date e.g. date(2020,10,1), date.today() - timedelta(days=1), etc.\n",
    "# from October 2020 to today which captures the majority of the sales\n",
    "\n",
    "download_sales_info(save_location = save_location, start_date = date(2021,7,30))\n",
    "\n",
    "# LISTINGS\n",
    "# download listings info from start_date to end_date e.g. date(2020,10,1), date.today() - timedelta(days=1), etc.\n",
    "# from October 2020 to today which captures the majority of the sales\n",
    "\n",
    "download_listings_info(save_location = save_location, start_date = date(2021,7,30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f293b0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the sales lists, combine them, and turn into a DF\n",
    "def load_sales_info(save_location):\n",
    "    files = [filename for filename in os.listdir(save_location) if filename.startswith('events_sales')]\n",
    "    all_sales = []\n",
    "    # load all files for sales by day\n",
    "    for file in files:\n",
    "        with open(str(save_location) + str(file), 'rb') as f:\n",
    "            all_sales.append(pickle.load(f))\n",
    "    \n",
    "    # flatten into one list\n",
    "    all_sales = [item for sublist in all_sales for item in sublist]\n",
    "    # convert to dataframe\n",
    "    events_sales_df = pd.DataFrame(all_sales)\n",
    "    \n",
    "    return events_sales_df\n",
    "\n",
    "# load the listing lists, combine them, and turn into a DF\n",
    "def load_listings_info(save_location):\n",
    "    files = [filename for filename in os.listdir(save_location) if filename.startswith('events_listing')]\n",
    "    all_listings = []\n",
    "    # load all files for listings by day\n",
    "    for file in files:\n",
    "        with open(str(save_location) + str(file), 'rb') as f:\n",
    "            all_listings.append(pickle.load(f))\n",
    "    \n",
    "    # flatten into one list\n",
    "    all_listings = [item for sublist in all_listings for item in sublist]\n",
    "    # convert to dataframe\n",
    "    events_listings_df = pd.DataFrame(all_listings)\n",
    "    \n",
    "    return events_listings_df\n",
    "\n",
    "# load most recent saved assets df\n",
    "def load_assets_info(save_location):\n",
    "    files = glob.glob(str(save_location)+'assets_df????-??-??.pkl')\n",
    "    with open(max(files, key=os.path.getctime), 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a036cfcf",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'save_location' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_21236/1551040261.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m#SALES\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mevents_sales_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_sales_info\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msave_location\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;31m# Pre-processing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# Convert price from WEI to ETH & for now get rid of bundles and duplicates(?)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'save_location' is not defined"
     ]
    }
   ],
   "source": [
    "#load all our saved files\n",
    "\n",
    "#SALES\n",
    "events_sales_df = load_sales_info(save_location)\n",
    "# Pre-processing\n",
    "# Convert price from WEI to ETH & for now get rid of bundles and duplicates(?)\n",
    "events_sales_df = events_sales_df[(events_sales_df['payment_token'] != 'USDC') & (events_sales_df['is_bundle'] == False)].copy()\n",
    "events_sales_df = events_sales_df.loc[events_sales_df.astype(str).drop_duplicates().index]\n",
    "events_sales_df['total_price'] = events_sales_df['total_price']/10.**18\n",
    "# Change timestamp to datetime\n",
    "events_sales_df['timestamp'] = pd.to_datetime(events_sales_df['timestamp'])\n",
    "# Calculating the sale prices in USD\n",
    "events_sales_df['total_price_usd'] = events_sales_df['total_price'] * events_sales_df['usd_price']\n",
    "\n",
    "\n",
    "#LISTINGS\n",
    "events_listings_df = load_listings_info(save_location)\n",
    "# Pre-processing\n",
    "# Convert price from WEI to ETH & for now get rid of bundles and duplicates(?)\n",
    "events_listings_df = events_listings_df[(events_listings_df['payment_token'] != 'USDC') & (events_listings_df['is_bundle'] == False)].copy()\n",
    "events_listings_df = events_listings_df.loc[events_listings_df.astype(str).drop_duplicates().index]\n",
    "events_listings_df['starting_price'] = events_listings_df['starting_price']/10.**18\n",
    "# Change timestamp to datetime\n",
    "events_listings_df['created_date'] = pd.to_datetime(events_listings_df['created_date'])\n",
    "# Calculating the sale prices in USD\n",
    "events_listings_df['total_price_usd'] = events_listings_df['starting_price'] * events_listings_df['usd_price']\n",
    "\n",
    "#ASSETS\n",
    "assets_df = load_assets_info(save_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1c77674c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the dataframes with the assets, sales, and listings as a csv files\n",
    "assets_df.to_csv(path_or_buf='Output_csv_data_files/nfts/assets.csv')\n",
    "events_sales_df.to_csv(path_or_buf='Output_csv_data_files/nfts/sales.csv')\n",
    "events_listings_df.to_csv(path_or_buf='Output_csv_data_files/nfts/listings.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2a5f36",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
