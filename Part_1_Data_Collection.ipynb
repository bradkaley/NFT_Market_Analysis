{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08815956",
   "metadata": {},
   "source": [
    "# NFT Market Analysis Project\n",
    "## Part 1: OpenSea NFT Data Collection & Initial Cleaning\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10be6d17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Bad key savefig.frameon in file C:\\Users\\bradl\\anaconda3\\envs\\pyvizenv\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle, line 421 ('savefig.frameon : True')\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.3.4/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n",
      "\n",
      "Bad key verbose.level in file C:\\Users\\bradl\\anaconda3\\envs\\pyvizenv\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle, line 472 ('verbose.level  : silent      # one of silent, helpful, debug, debug-annoying')\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.3.4/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n",
      "\n",
      "Bad key verbose.fileo in file C:\\Users\\bradl\\anaconda3\\envs\\pyvizenv\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle, line 473 ('verbose.fileo  : sys.stdout  # a log filename, sys.stdout or sys.stderr')\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.3.4/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n",
      "In C:\\Users\\bradl\\anaconda3\\envs\\pyvizenv\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
      "The text.latex.preview rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In C:\\Users\\bradl\\anaconda3\\envs\\pyvizenv\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
      "The mathtext.fallback_to_cm rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In C:\\Users\\bradl\\anaconda3\\envs\\pyvizenv\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: Support for setting the 'mathtext.fallback_to_cm' rcParam is deprecated since 3.3 and will be removed two minor releases later; use 'mathtext.fallback : 'cm' instead.\n",
      "In C:\\Users\\bradl\\anaconda3\\envs\\pyvizenv\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
      "The validate_bool_maybe_none function was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In C:\\Users\\bradl\\anaconda3\\envs\\pyvizenv\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
      "The savefig.jpeg_quality rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In C:\\Users\\bradl\\anaconda3\\envs\\pyvizenv\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
      "The keymap.all_axes rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In C:\\Users\\bradl\\anaconda3\\envs\\pyvizenv\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
      "The animation.avconv_path rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In C:\\Users\\bradl\\anaconda3\\envs\\pyvizenv\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
      "The animation.avconv_args rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n"
     ]
    }
   ],
   "source": [
    "#Initial imports\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import alpaca_trade_api as tradeapi\n",
    "import json\n",
    "import csv\n",
    "\n",
    "\n",
    "#Imports required for pulling data using OpenSea API and cleaning the data\n",
    "from helpers import parse_events_data, parse_assets_data, parse_sale_data, parse_listing_data\n",
    "#from pandas_profiling import ProfileReport\n",
    "import pickle\n",
    "from statistics import *\n",
    "from scipy.stats import combine_pvalues\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "plt.style.use('ggplot')\n",
    "import time\n",
    "from datetime import date, timedelta, datetime\n",
    "import glob\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a4bd182",
   "metadata": {},
   "source": [
    "## Pull OpenSea data using API, save in Pandas dataframe, and clean the data\n",
    "\n",
    "This section contains code pulled from Alex Duffy's Github which references base code by Adil Moujahid. \\\n",
    "Blog post located here: https://alxdfy.github.io/2021/09/19/data-mining-OpenSea_markdown.html#get-nft \\\n",
    "GitHub repository located here: https://github.com/AlxDfy/OpenSea_API_DataScience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9be2948",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Set a variable equal to the smart contract address for the desired NFT collection\n",
    "Here we'll be evaluating 3 of the most popular (by volume) collections as of 11/16/2021: Bored Ape Yacht Club, Mutant Ape Yacht Club, and The Sandbox.\n",
    "Each collection's smart contract address is swapped in for the \"asset_conctract_address variable to pull the data using the API and then saved as csv files. The smart contract addresses are as follows:\n",
    "Bored Ape Yacht Club: 0xBC4CA0EdA7647A8aB7C2061c2E118A18a936f13D\n",
    "Mutant Ape Yacht Club: 0x60E4d786628Fea6478F785A6d7e704777c86a7c6\n",
    "The Sandbox: 0x50f5474724e0Ee42D9a4e711ccFB275809Fd6d4a\n",
    "\"\"\"\n",
    "\n",
    "asset_contract_address = \"0xBC4CA0EdA7647A8aB7C2061c2E118A18a936f13D\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b6e47bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define function to pull NFT properties data from Opensea\n",
    "\n",
    "def download_asset_info(save_location):\n",
    "    if not os.path.isdir(save_location):\n",
    "        os.makedirs(save_location)\n",
    "    url = \"https://api.opensea.io/api/v1/assets\"\n",
    "    listoassets = []\n",
    "\n",
    "    for i in range(0, 3000):\n",
    "        querystring = {\"token_ids\":list(range((i*30), (i*30)+30)),\n",
    "                       \"asset_contract_address\":asset_contract_address,\n",
    "                       \"order_direction\":\"desc\",\n",
    "                       \"offset\":\"0\",\n",
    "                       \"limit\":\"30\"}\n",
    "        response = requests.request(\"GET\", url, params=querystring)\n",
    "\n",
    "        print(i, end=\" \")\n",
    "        if response.status_code != 200:\n",
    "            print('error')\n",
    "            print(response.json())\n",
    "            break\n",
    "\n",
    "        # Getting assets data\n",
    "        assets = response.json()['assets']\n",
    "        if assets == []:\n",
    "            break\n",
    "        # Parsing assets data\n",
    "        parsed_assets = [parse_assets_data(asset) for asset in assets]\n",
    "        # storing parsed events data into list\n",
    "        listoassets.append(parsed_assets)\n",
    "    \n",
    "    # Flatten everything into one list\n",
    "    listoassets = [item for sublist in listoassets for item in sublist]\n",
    "    # Convert to df\n",
    "    assets_df = pd.DataFrame(listoassets)\n",
    "\n",
    "    #Save data in a new file\n",
    "    with open(save_location + 'assets_df'+str(date.today())+r'.pkl', 'wb') as f:\n",
    "        pickle.dump(assets_df, f)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e83a10c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define function to pull NFT sales data from Opensea\n",
    "# Download sales info from start_date to end _date and save them all into their own day's files\n",
    "# Default values from October 2020 to today which captures the majority of the sales\n",
    "\n",
    "def download_sales_info(save_location, start_date = date(2020,10,30), end_date = date.today()):\n",
    "    if not os.path.isdir(save_location):\n",
    "        os.makedirs(save_location)\n",
    "    url = \"https://api.opensea.io/api/v1/events\"\n",
    "    # get the number of days that we want to download and save sales for\n",
    "    delta = end_date - start_date\n",
    "    count_days = int(delta.days)\n",
    "    \n",
    "    for i in range(count_days+1):\n",
    "        sales_that_day = []\n",
    "        # set start and end of the day we are checking, if it's today set end to current time\n",
    "        if date.today() == (start_date + timedelta(days=i)):\n",
    "            before = datetime.now()\n",
    "            after = datetime.combine((start_date + timedelta(days=i)), datetime.min.time())\n",
    "        else:\n",
    "            before = datetime.combine((start_date + timedelta(days=i+1)), datetime.min.time())\n",
    "            after = datetime.combine((start_date + timedelta(days=i)), datetime.min.time())\n",
    "        # There are too many transactions, now have to break them up by chunks in the day\n",
    "        hour_chunks = 24\n",
    "        chunk_count = 24/hour_chunks\n",
    "        time.sleep(.5)\n",
    "\n",
    "        for chunk in range(int(chunk_count)):\n",
    "            end = False\n",
    "            for j in range(0, 35):\n",
    "                time.sleep(.5)\n",
    "                # add the hour_chunk to the start of the day (after) time for each chunk\n",
    "                # use the actual before if we pass it chronologically though\n",
    "                changed_before = after + timedelta(hours=hour_chunks*(chunk+1)) - timedelta(minutes = 1)\n",
    "                changed_after = after + timedelta(hours = hour_chunks*(chunk))\n",
    "                \n",
    "                # this should only happen on the last chunk of a split day or if on current day\n",
    "                if before < changed_before:\n",
    "                    changed_before = before\n",
    "                    end = True\n",
    "\n",
    "                querystring = {\"asset_contract_address\":asset_contract_address,\n",
    "                               \"event_type\":\"successful\",\n",
    "                               \"only_opensea\":\"true\",\n",
    "                               \"offset\":j*300,\n",
    "                               \"occurred_before\":changed_before,\n",
    "                               \"occurred_after\":changed_after,\n",
    "                               \"limit\":\"300\"}\n",
    "                headers = {\"Accept\": \"application/json\"}\n",
    "\n",
    "                response = requests.request(\"GET\", url, headers=headers, params=querystring)\n",
    "\n",
    "\n",
    "                print(j, end=\" \")\n",
    "                if response.status_code != 200:\n",
    "                    print('error')\n",
    "                    print(response.json())\n",
    "                    break\n",
    "\n",
    "                #Getting assets sales data\n",
    "                event_sales = response.json()['asset_events']\n",
    "\n",
    "                if event_sales == []:\n",
    "                    end =True\n",
    "                    break\n",
    "\n",
    "                # Parsing asset sales data\n",
    "                parsed_event_sales = [parse_sale_data(sale) for sale in event_sales]\n",
    "                # storing parsed events data into list\n",
    "                sales_that_day.append(parsed_event_sales)\n",
    "                # check if the last date in the list is the same day as \n",
    "                last_date = (datetime.strptime(parsed_event_sales[0]['timestamp'], '%Y-%m-%dT%H:%M:%S'))\n",
    "                print(last_date)\n",
    "            if end:\n",
    "                break\n",
    "        sales_that_day = [item for sublist in sales_that_day for item in sublist]\n",
    "        \n",
    "        print(str(len(sales_that_day))+ \" sales saved to\" + save_location + \"events_sales_list_\" + str(start_date + timedelta(days=i))+'.pkl')\n",
    "        with open(save_location + \"events_sales_list_\" + str(start_date + timedelta(days=i))+'.pkl', 'wb') as f:\n",
    "            pickle.dump(sales_that_day, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2eb2ae24",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define function to pull NFT listings data from Opensea\n",
    "\n",
    "# Download listings info from start_date to end _date and save them all into their own day's files\n",
    "# Default values from October 2020 to today which captures the majority of the sales\n",
    "def download_listings_info(save_location, start_date = date(2020,10,30), end_date = date.today()):\n",
    "    if not os.path.isdir(save_location):\n",
    "        os.makedirs(save_location)\n",
    "    url = \"https://api.opensea.io/api/v1/events\"\n",
    "    # get the number of days that we want to download and save listings for\n",
    "    delta = end_date - start_date\n",
    "    count_days = int(delta.days)\n",
    "    \n",
    "    for i in range(count_days+1):\n",
    "        listings_that_day = []\n",
    "        # set start and end of the day we are checking, if it's today set end to current time\n",
    "        if date.today() == (start_date + timedelta(days=i)):\n",
    "            before = datetime.now()\n",
    "            after = datetime.combine((start_date + timedelta(days=i)), datetime.min.time())\n",
    "        else:\n",
    "            before = datetime.combine((start_date + timedelta(days=i+1)), datetime.min.time())\n",
    "            after = datetime.combine((start_date + timedelta(days=i)), datetime.min.time())\n",
    "        # There are too many transactions, now have to break them up by chunks in the day\n",
    "        hour_chunks = 24\n",
    "        chunk_count = 24/hour_chunks\n",
    "        time.sleep(.5)\n",
    "\n",
    "        \n",
    "        for chunk in range(int(chunk_count)):\n",
    "            end = False\n",
    "            for j in range(0, 35):\n",
    "                time.sleep(.5)\n",
    "                # add the hour_chunk to the start of the day (after) time for each chunk\n",
    "                # use the actual before if we pass it chronologically though\n",
    "                changed_before = after + timedelta(hours=hour_chunks*(chunk+1)) - timedelta(minutes = 1)\n",
    "                changed_after = after + timedelta(hours = hour_chunks*(chunk))\n",
    "                \n",
    "                # this should only happen on the last chunk of a split day or if on current day\n",
    "                if before < changed_before:\n",
    "                    changed_before = before\n",
    "                    end = True\n",
    "\n",
    "                querystring = {\"asset_contract_address\":asset_contract_address,\n",
    "                               \"event_type\":\"created\",\n",
    "                               \"only_opensea\":\"true\",\n",
    "                               \"offset\":j*300,\n",
    "                               \"occurred_before\":changed_before,\n",
    "                               \"occurred_after\":changed_after,\n",
    "                               \"limit\":\"300\"}\n",
    "                headers = {\"Accept\": \"application/json\"}\n",
    "\n",
    "                response = requests.request(\"GET\", url, headers=headers, params=querystring)\n",
    "\n",
    "\n",
    "                print(j, end=\" \")\n",
    "                if response.status_code != 200:\n",
    "                    print('error')\n",
    "                    print(response.json())\n",
    "                    break\n",
    "\n",
    "                #Getting assets listings data\n",
    "                event_listings = response.json()['asset_events']\n",
    "\n",
    "                if event_listings == []:\n",
    "                    end = True\n",
    "                    break\n",
    "\n",
    "                # Parsing events listings data\n",
    "                parsed_event_listings = [parse_listing_data(listing) for listing in event_listings]\n",
    "                # storing parsed events data into list\n",
    "                listings_that_day.append(parsed_event_listings)\n",
    "                # check if the last date in the list is the same day as \n",
    "                print(parsed_event_listings[0]['created_date'])\n",
    "            if end:\n",
    "                break\n",
    "        listings_that_day = [item for sublist in listings_that_day for item in sublist]\n",
    "        \n",
    "        print(str(len(listings_that_day))+ \" listings saved to\" + save_location +\n",
    "              \"events_listings_list_\" + str(start_date + timedelta(days=i))+'.pkl')\n",
    "        with open(save_location + \"events_listings_list_\" + str(start_date + timedelta(days=i))+'.pkl', 'wb') as f:\n",
    "            pickle.dump(listings_that_day, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a5de5f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1 2 "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-dfa49b638b46>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# assets\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mdownload_asset_info\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msave_location\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m# SALES\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-752799400a13>\u001b[0m in \u001b[0;36mdownload_asset_info\u001b[1;34m(save_location)\u001b[0m\n\u001b[0;32m     13\u001b[0m                        \u001b[1;34m\"offset\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;34m\"0\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m                        \"limit\":\"30\"}\n\u001b[1;32m---> 15\u001b[1;33m         \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"GET\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mquerystring\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\" \"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pyvizenv\\lib\\site-packages\\requests\\api.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[1;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pyvizenv\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    540\u001b[0m         }\n\u001b[0;32m    541\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 542\u001b[1;33m         \u001b[0mresp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    543\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pyvizenv\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    653\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    654\u001b[0m         \u001b[1;31m# Send the request\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 655\u001b[1;33m         \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    656\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    657\u001b[0m         \u001b[1;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pyvizenv\\lib\\site-packages\\requests\\adapters.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    447\u001b[0m                     \u001b[0mdecode_content\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    448\u001b[0m                     \u001b[0mretries\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_retries\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 449\u001b[1;33m                     \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    450\u001b[0m                 )\n\u001b[0;32m    451\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pyvizenv\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    704\u001b[0m                 \u001b[0mbody\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    705\u001b[0m                 \u001b[0mheaders\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 706\u001b[1;33m                 \u001b[0mchunked\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mchunked\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    707\u001b[0m             )\n\u001b[0;32m    708\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pyvizenv\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    443\u001b[0m                     \u001b[1;31m# Python 3 (including for exceptions like SystemExit).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    444\u001b[0m                     \u001b[1;31m# Otherwise it looks like a bug in the code.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 445\u001b[1;33m                     \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    446\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mSocketTimeout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBaseSSLError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSocketError\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    447\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_raise_timeout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout_value\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mread_timeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pyvizenv\\lib\\site-packages\\urllib3\\packages\\six.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pyvizenv\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    438\u001b[0m                 \u001b[1;31m# Python 3\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    439\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 440\u001b[1;33m                     \u001b[0mhttplib_response\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    441\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    442\u001b[0m                     \u001b[1;31m# Remove the TypeError from the exception chain in\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pyvizenv\\lib\\http\\client.py\u001b[0m in \u001b[0;36mgetresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1367\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1368\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1369\u001b[1;33m                 \u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1370\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1371\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pyvizenv\\lib\\http\\client.py\u001b[0m in \u001b[0;36mbegin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    308\u001b[0m         \u001b[1;31m# read until we get a non-100 response\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    309\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 310\u001b[1;33m             \u001b[0mversion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    311\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    312\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pyvizenv\\lib\\http\\client.py\u001b[0m in \u001b[0;36m_read_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    269\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    270\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_read_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 271\u001b[1;33m         \u001b[0mline\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"iso-8859-1\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    272\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    273\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mLineTooLong\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"status line\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pyvizenv\\lib\\socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    587\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    588\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 589\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    590\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    591\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pyvizenv\\lib\\ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1069\u001b[0m                   \u001b[1;34m\"non-zero flags not allowed in calls to recv_into() on %s\"\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1070\u001b[0m                   self.__class__)\n\u001b[1;32m-> 1071\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1072\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1073\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pyvizenv\\lib\\ssl.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m    927\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    928\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 929\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    930\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Execute the pull requests for the properties and sales data for the NFTs\n",
    "\n",
    "# Change save location and/or the name of the collection depending on which NFT collection being analyzed\n",
    "save_location = \"./static/mutant_ape/\"\n",
    "\n",
    "# assets\n",
    "download_asset_info(save_location)\n",
    "\n",
    "# SALES\n",
    "# download sales info from start_date to end_date e.g. date(2020,10,1), date.today() - timedelta(days=1), etc.\n",
    "# from October 2020 to today which captures the majority of the sales\n",
    "\n",
    "download_sales_info(save_location = save_location, start_date = date(2020,10,30))\n",
    "\n",
    "# LISTINGS\n",
    "# download listings info from start_date to end_date e.g. date(2020,10,1), date.today() - timedelta(days=1), etc.\n",
    "# from October 2020 to today which captures the majority of the sales\n",
    "\n",
    "download_listings_info(save_location = save_location, start_date = date(2020,10,30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6de14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the sales lists, combine them, and turn into a DF\n",
    "def load_sales_info(save_location):\n",
    "    files = [filename for filename in os.listdir(save_location) if filename.startswith('events_sales')]\n",
    "    all_sales = []\n",
    "    # load all files for sales by day\n",
    "    for file in files:\n",
    "        with open(str(save_location) + str(file), 'rb') as f:\n",
    "            all_sales.append(pickle.load(f))\n",
    "    \n",
    "    # flatten into one list\n",
    "    all_sales = [item for sublist in all_sales for item in sublist]\n",
    "    # convert to dataframe\n",
    "    events_sales_df = pd.DataFrame(all_sales)\n",
    "    \n",
    "    return events_sales_df\n",
    "\n",
    "# load the listing lists, combine them, and turn into a DF\n",
    "def load_listings_info(save_location):\n",
    "    files = [filename for filename in os.listdir(save_location) if filename.startswith('events_listing')]\n",
    "    all_listings = []\n",
    "    # load all files for listings by day\n",
    "    for file in files:\n",
    "        with open(str(save_location) + str(file), 'rb') as f:\n",
    "            all_listings.append(pickle.load(f))\n",
    "    \n",
    "    # flatten into one list\n",
    "    all_listings = [item for sublist in all_listings for item in sublist]\n",
    "    # convert to dataframe\n",
    "    events_listings_df = pd.DataFrame(all_listings)\n",
    "    \n",
    "    return events_listings_df\n",
    "\n",
    "# load most recent saved assets df\n",
    "def load_assets_info(save_location):\n",
    "    files = glob.glob(str(save_location)+'assets_df????-??-??.pkl')\n",
    "    with open(max(files, key=os.path.getctime), 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed669cca",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'save_location' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_21236/1551040261.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m#SALES\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mevents_sales_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_sales_info\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msave_location\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;31m# Pre-processing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# Convert price from WEI to ETH & for now get rid of bundles and duplicates(?)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'save_location' is not defined"
     ]
    }
   ],
   "source": [
    "# Load all our saved files\n",
    "\n",
    "#SALES\n",
    "events_sales_df = load_sales_info(save_location)\n",
    "# Pre-processing\n",
    "# Convert price from WEI to ETH & for now get rid of bundles and duplicates(?)\n",
    "events_sales_df = events_sales_df[(events_sales_df['payment_token'] != 'USDC') & (events_sales_df['is_bundle'] == False)].copy()\n",
    "events_sales_df = events_sales_df.loc[events_sales_df.astype(str).drop_duplicates().index]\n",
    "events_sales_df['total_price'] = events_sales_df['total_price']/10.**18\n",
    "# Change timestamp to datetime\n",
    "events_sales_df['timestamp'] = pd.to_datetime(events_sales_df['timestamp'])\n",
    "# Calculating the sale prices in USD\n",
    "events_sales_df['total_price_usd'] = events_sales_df['total_price'] * events_sales_df['usd_price']\n",
    "\n",
    "\n",
    "#LISTINGS\n",
    "events_listings_df = load_listings_info(save_location)\n",
    "# Pre-processing\n",
    "# Convert price from WEI to ETH & for now get rid of bundles and duplicates(?)\n",
    "events_listings_df = events_listings_df[(events_listings_df['payment_token'] != 'USDC') & (events_listings_df['is_bundle'] == False)].copy()\n",
    "events_listings_df = events_listings_df.loc[events_listings_df.astype(str).drop_duplicates().index]\n",
    "events_listings_df['starting_price'] = events_listings_df['starting_price']/10.**18\n",
    "# Change timestamp to datetime\n",
    "events_listings_df['created_date'] = pd.to_datetime(events_listings_df['created_date'])\n",
    "# Calculating the sale prices in USD\n",
    "events_listings_df['total_price_usd'] = events_listings_df['starting_price'] * events_listings_df['usd_price']\n",
    "\n",
    "#ASSETS\n",
    "assets_df = load_assets_info(save_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1ddb8a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the dataframes with the assets, sales, and listings as new csv files, then change the name of the file as needed once it's saved\n",
    "assets_df.to_csv(path_or_buf='Output_csv_data_files/nfts/assets.csv')\n",
    "events_sales_df.to_csv(path_or_buf='Output_csv_data_files/nfts/sales.csv')\n",
    "events_listings_df.to_csv(path_or_buf='Output_csv_data_files/nfts/listings.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a26fe1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
